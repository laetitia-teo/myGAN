import os
import numpy as np
import tensorflow as tf
from ops import *
import matplotlib.pyplot as plt


class GAN():
    """
    GAN class, contains G (generative) and D (discriminative) networks.
    """    
    def __init__(self, dataset, load_model=False, k=1, m=1):
        
        self.batch_size = 64
        self.dataset = dataset # must support batch iteration
        self.dataset.batch_size = self.batch_size
        # hyperparams
        self.z_dim = 50
        self.k = k # number of discriminative optimization steps
        self.m = m # number of generative optimization steps
        
        self.zbatch = tf.placeholder(tf.float32, [None, self.z_dim])
        self.images = tf.placeholder(tf.float32, [None, 28, 28, 1])
        
        self.hidden = 100 # do we need them ?
        
        # models
        # images is the generator, it outputs images from the noise vector z.
        # But images is also the entry point for the original images.
        # D is the discriminator
        
        eps = 10e-8
        
        with tf.variable_scope("generator"):
            self.G = self.gen(self.zbatch)
        
        with tf.variable_scope("discriminator") as scope:
            self.D = tf.clip_by_value(self.discr(self.images), eps, 1 - eps)
            scope.reuse_variables() # reusing the discriminator on the generator
            self.Dg = tf.clip_by_value(self.discr(self.G), eps, 1 - eps)
        
        # losses :
        # careful, there's a minus sign
        self.lossD = - (tf.reduce_sum(tf.log(self.D), 1)\
                     + tf.reduce_sum(tf.log(1 - self.Dg), 1))
        
        self.lossG = - tf.reduce_sum(tf.log(self.Dg), 1)
        
        # optimizers :
        self.optD = tf.train.AdamOptimizer(0.002).minimize(self.lossD)
        self.optG = tf.train.AdamOptimizer(0.002).minimize(self.lossG)
        
        # save path
        self.save_path = os.getcwd() + "/saves/model"
        self.load_model = load_model
        
        # session
    
    def plot_images(self, images):
        """
        Utility function for plotting 8x8 grids of mnist images.
        
        Args : 
            - images (numpy ndarray) : array of 64 mnist images for plotting. 
        """
        for i in range(8):
            for j in range(8):
                if j == 0:
                    row = images[8*i+j]
                else:
                    row = np.concatenate((row, images[8*i+j]), axis=1)
            if i == 0:
                stack = row
            else:
                stack = np.concatenate((stack, row), axis=0)
        plt.imshow(stack, cmap='gray')
        plt.show()
    
    def plot_gen(self):
        """
        Plots a grid of 8x8 images generated by the generator.
        """
        saver = tf.train.Saver()
        with tf.Session() as sess:
            if self.load_model:
                print('new session')
                sess.run(tf.global_variables_initializer())
            else:
                try:
                    saver.restore(sess, self.save_path)
                    print('loaded model at %s' % self.save_path)
                except:
                    print('no model was found to load, initializing new session')
                    sess.run(tf.global_variables_initializer())
            zbatch = self.sample_z()
            gen_images = sess.run(self.G, 
                                  feed_dict={self.zbatch: self.sample_z()})
        self.plot_images(gen_images[:, :, :, 0])
    
    def sample_z(self):
        """
        Function for generation of a random vector of noise in latent space, 
        with a gaussian prior.
        
        Args : 
            - n_samples (int) : number of samples to draw.
        """
        return np.random.normal(size=[self.batch_size, self.z_dim])
        
    def gen(self, z):
        """
        The generative network. Given a vector of noise z, it generates an 
        image according to p_g(z). We want this distribution to be as close as
        possible to the data distribution p_data. 
        """
        z_expand1 = dense(z, 7*7*64, "expand1")
        z_matrix = tf.nn.relu(tf.reshape(z_expand1, [self.batch_size, 7, 7, 64]))
        deconv1 = tf.nn.relu(deconv2d(z_matrix, 5, 
            [self.batch_size, 14, 14, 32], "deconv1"))
        deconv2 = deconv2d(deconv1, 5, [self.batch_size, 28, 28, 1], "deconv2")
        gen_image = tf.nn.sigmoid(deconv2)
        
        return gen_image
    
    def discr(self, images):
        """
        The discriminative network. Given a batch of images, it has to 
        discriminate between the ones generated by the generator and the ones
        from the dataset.
        """
        conv1 = tf.nn.relu(conv2d(images, 5, 16, "conv1"))
        conv2 = tf.nn.relu(conv2d(conv1, 5, 32, "conv2"))
        flat = tf.reshape(conv2, [self.batch_size, 7*7*32])
        
        #dense = tf.nn.relu(dense(flat, self.hidden, "hidden"))
        proba = tf.nn.sigmoid(dense(flat, 1, "dense_sigma"))
        
        return proba
    
    def train_D(self, sess, batch, zbatch):
        """
        Training function for the discriminator objective.
        """
        return sess.run((self.optD, self.lossD),
                         feed_dict={self.images: batch,
                                    self.zbatch: zbatch})
    
    def train_G(self, sess, zbatch):
        """
        Training function for the generator objective.
        """      
        return sess.run((self.optG, self.lossG),
                         feed_dict={self.zbatch: zbatch})
                     
    def train(self, n_epochs):
        """
        Outer training loop.
        """
        saver = tf.train.Saver()
        with tf.Session() as sess:
            if self.load_model:
                try:
                    saver.restore(sess, self.save_path)
                    print('loaded model at %s' % self.save_path)
                except:
                    print('no model was found to load, initializing new '
                        + 'session')
                    sess.run(tf.global_variables_initializer())
            else:
                # no model to load, initializing session
                sess.run(tf.global_variables_initializer())
                self.load_model = True
            for epoch in range(n_epochs):
                print('epoch %s' % epoch)
                data_iter = iter(self.dataset)
                while True:
                    # iterate on the dataset, stop when the iterator stops
                    try:
                        for k in range(self.k):
                            # k runs of optimization on the discriminator
                            batch = next(data_iter)
                            zbatch = self.sample_z()
                            _, lossD = self.train_D(sess, batch, zbatch)
                            print('lossD : %s' % lossD)
                        for m in range(self.m):
                            # m runs of optimization on the generator
                            zbatch = self.sample_z()
                            _, lossG = self.train_G(sess, zbatch)
                            print('lossG : %s' % lossG)
                    except(StopIteration):
                        break
            save_path = saver.save(sess, self.save_path)
            print('Model saved in path : %s' % save_path)
                






















    
    
