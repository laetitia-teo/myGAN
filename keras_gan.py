import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from keras_models import deep_gen, deep_discr

class GAN():
    """
    GAN class, contains G (generative) and D (discriminative) networks.
    """    
    def __init__(self, dataset, load_model=False, k=1, m=1):
        
        if (tf.__version__ != '2.0.0-alpha0'):
            tf.enable_eager_execution()
        
        self.batch_size = 64
        self.dataset = dataset # must support batch iteration
        self.dataset.batch_size = self.batch_size
        # hyperparams
        self.z_dim = 100
        self.k = k # number of discriminative optimization steps
        self.m = m # number of generative optimization steps
        
        self.gen = self.make_gen(self.z_dim)
        self.discr = self.make_discr()
        
        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(
            from_logits=True)
        
        self.gen_opt = tf.keras.optimizers.Adam(1e-4)
        self.discr_opt = tf.keras.optimizers.Adam(1e-4)
        """
        self.save_path = os.getcwd() + "/saves/keras"
        self.checkpoint_prefix = os.path.join(self.save_path, "ckpt")
        self.checkpoint = tf.train.Checkpoint(gen_opt=self.gen_opt,
                                              discr_opt=self.discr_opt,
                                              gen=self.gen,
                                              discr=self.discr)
        """

    def plot_images(self, images):
        """
        Utility function for plotting 8x8 grids of mnist images.
        
        Args : 
            - images (numpy ndarray) : array of 64 mnist images for plotting. 
        """
        for i in range(8):
            for j in range(8):
                if j == 0:
                    row = images[8*i+j]
                else:
                    row = np.concatenate((row, images[8*i+j]), axis=1)
            if i == 0:
                stack = row
            else:
                stack = np.concatenate((stack, row), axis=0)
        plt.imshow(stack, cmap='gray')
        plt.show()
    
    def plot_gen(self):
        """
        Plots a grid of 8x8 images generated by the generator.
        """
        #self.training = False
        #saver = tf.train.Saver()
        gen_images = self.gen(self.sample_z(), training=False)
        #print(gen_images)
        self.plot_images(gen_images[:, :, :, 0])
        self.training = True
    
    def sample_z(self):
        """
        Function for generation of a random vector of noise in latent space, 
        with a gaussian prior.
        
        Args : 
            - n_samples (int) : number of samples to draw.
        """
        return tf.random.normal([self.batch_size, self.z_dim])
    
    def make_gen(self, z_dim):
        """
        Returns the generator model.
        """
        return deep_gen(z_dim)
    
    def make_discr(self):
        """
        Returns the discriminator model.
        """
        return deep_discr()
    
    def gen_loss(self, gen_discr):
        """
        Returns generator loss.
        """
        return self.cross_entropy(tf.ones_like(gen_discr), gen_discr)
    
    def discr_loss(self, true_discr, gen_discr):
        """
        Returns discriminator loss.
        """
        true_loss = self.cross_entropy(tf.ones_like(true_discr), true_discr)
        gen_loss = self.cross_entropy(tf.zeros_like(gen_discr), gen_discr)
        return true_loss + gen_loss
    
    def train(self, n_epochs):
        """
        Performs training on the GAN for n_epochs iterations on the dataset.
        """
        for i in range(n_epochs):
            for im_batch in self.dataset:
                z_batch = self.sample_z()
                with tf.GradientTape() as gen_tape, tf.GradientTape() as discr_tape:
                
                    gen_ims = self.gen(z_batch, training=True)
                    
                    true_discr = self.discr(im_batch, training=True)
                    gen_discr = self.discr(gen_ims, training=True)
                    
                    gen_loss = self.gen_loss(gen_discr)
                    discr_loss = self.discr_loss(true_discr, gen_discr)
                
                gen_vars = self.gen.trainable_variables
                discr_vars = self.discr.trainable_variables
                
                gen_grads = gen_tape.gradient(gen_loss, gen_vars)
                discr_grads = discr_tape.gradient(discr_loss, discr_vars)
                
                print('gen_loss, discr_loss %s, %s' % (gen_loss, discr_loss))
                
                self.gen_opt.apply_gradients(zip(gen_grads, gen_vars))
                self.discr_opt.apply_gradients(zip(discr_grads, discr_vars))
                
        #checkpoint.save(file_prefix = self.checkpoint_prefix)
            







































